{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder und Generative Models \n",
    "\n",
    "Diese Kapitel gibt eine kurze Einführung in aktuelle Forschungsaktivitäten im Bereich des Deep Learning. Wir stellen dabei die beiden Themen \n",
    "\n",
    "- Autoencoder \n",
    "- Generative Models am Beispiel der Variational Autoencoder \n",
    "\n",
    "vor. Zur Behandlung beider Themen empfiehlt sich die sogannnte Functional API (Model aus keras.models). In dieser API können Layer als Funktionen (daher der Name Functional API) verwendet werden, die als Eingabe und Ausgabe Tensoren besitzen. Die Functional API erlaubt es zudem Tensoren auch innerhalb des Netzwerkes auszuwerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API in Keras\n",
    "\n",
    "Mit der Functional API von Keras können layer als Funktionen aufgefasst werden, die als Input Tensoren erhalten und als Output Tensoren  zurückgeben. Die Functional API findet sich unter keras.models als Model:\n",
    "\n",
    "https://keras.io/guides/functional_api/\n",
    "\n",
    "Wir schauen dieses Vorgehen am Beispiel eines neuronalen Netzes mit zwei Hidden-Layer an (zur Illustration bauen wir diese Netz nochmals als Sequential model auf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential # Sequential model API\n",
    "from keras.models import Model      # Functional model API\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, Lambda\n",
    "\n",
    "from keras import Input\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronales Netz mit zwei Zwischenschichten\n",
    "n  = 784\n",
    "h1 = 100\n",
    "h2 = 100\n",
    "K = 10\n",
    "\n",
    "# Sequential model API\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(h1,activation='relu',input_shape=(784,)))\n",
    "model1.add(Dense(h2,activation='relu'))\n",
    "model1.add(Dense(K,activation='softmax'))\n",
    "#model1.summary()\n",
    "\n",
    "# Functional model API\n",
    "In  = Input(shape=(784,))\n",
    "H1  = Dense(h1,activation='relu')(In)\n",
    "H2  = Dense(h2,activation='relu')(H1)\n",
    "Out = Dense(K,activation='softmax')(H2)\n",
    "\n",
    "model2 = Model(In,Out)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sind wir beispielsweise an der Datenrepräsentation $H_2$ nach dem Trainig interresiert, so können wir mir der Functional API ein Netz erstellen, welches die inputs \"In\" auf $H_2$ abbildet. Bezeichnen wir dieses Netz als coding, so ergibt sich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding = Model(In,H2)\n",
    "coding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir trainieren das model2 mit den MNIST Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(60000,28*28 )\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(10000,28*28 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model2.fit(x_train,y_train,epochs=10,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auslesen von H2\n",
    "coding.predict(x_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "Sogenannte **Autoencoder** zählen zu den unsupervised Learning Algorithmen. Sie bestehen aus künstlichen neuronales Netzen, welche u.a. dazu verwendet werden, effiziente Codierungen zu lernen. Ein Autoencoders hat zum Ziel, eine komprimierte Repräsentation (sogenanntes Encoding) für einen Datensatz zu lernen und somit wesentliche Merkmale zu extrahieren (sogenannte Feature Extraction). \n",
    "\n",
    "Das übliche Vorgehen bei Autoencodern ist wie folgt\n",
    "\n",
    "- Man bildet einen (oft hochdimensionalen) Input mittels einer oder mehrerer Zwischenschichten (diese können auch aus Convolutional Layern bestehen) auf einen niederdimensionalen Zwischenlayer ab (dieser enthält das oben angesprochene Encoding des Inputs). Diese Teil des neuronalen Netzes wird **Encoder** genannt.\n",
    "- Ausgehend von diesem Encoding wird mittels einer oder mehrerer Zwischenschichten wieder auf den Output abgebildet. Dieser Teil des Netzes wird **Decoder** genannt. \n",
    "\n",
    "Beide Vorgänge sind in der folgenden Abbildung illustriert:\n",
    "\n",
    "<img src=\"autoencoder.png\" height=\"100\" width=\"600\"/>\n",
    "\n",
    "Bestehen Encoder/Decoder aus vielen (Convolutional) Layern, so spricht man von **Deep Autoencoder**.\n",
    "Die Cosfunction eines Autoencoders ist im einfachen Falle wie folgt konstruiert:\n",
    "$$\n",
    "J(\\mathbf{W}) = \\frac{1}{2m}\\sum_{i=1}^m\\left(\\vec{x}^{(i)}-\\vec{x}_{p}^{(i)}\\right)^2\n",
    "$$\n",
    "Hierbei ist $\\vec{x}^{(i)}$ der Input von Training Example $i$ (in der Illustration oben die das vektorisierte Bild der Zahl 4 links). $\\vec{x}_{p}^{(i)}$ bezeichnet den Output des  Encoder/Decoder-Netzwerkes (in der Illustration oben die das vektorisierte Bild der Zahl 4 rechts). Dieser Output hängt von der Gesamtheit aller Gewichte $\\mathbf{W}$ ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellung eines einfachen Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder \n",
    "In  = Input(shape=(784,))\n",
    "z  = Dense(2)(In)\n",
    "\n",
    "encoder = Model(In,z)\n",
    "\n",
    "# Decoder \n",
    "dec_In = Input(K.int_shape(z)[1:])\n",
    "Out = Dense(784)(dec_In)\n",
    "\n",
    "decoder = Model(dec_In,Out)\n",
    "\n",
    "z_decoded = decoder(z)\n",
    "\n",
    "model = Model(In,z_decoded)\n",
    "#model.summary()\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "#model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "#model.fit(x_train,x_train,epochs=10,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In  = Input(shape=(784,))\n",
    "z   = Dense(100)(In)\n",
    "Out = Dense(784)(z) \n",
    "\n",
    "\n",
    "model = Model(In,Out)\n",
    "#model.summary()\n",
    "encoder = Model(In,z)\n",
    "#encoder.summary()\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "model.fit(x_train,x_train,epochs=10,batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(encoder.predict(x_test)[:,2],encoder.predict(x_test)[:,80],c=y_test,cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2\n",
    "plt.imshow(model.predict(x_test)[L,:].reshape(28,28),cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(x_test[L,:].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(model.predict(x_test)[L,:].reshape(28,28),cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_train[L,:].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellung eines Deep Autoencoders\n",
    "\n",
    "Obig vorgestellter einfacher Ansatz eines Autoencoders führt im Falle einer linearen Aktivierung in der Zwischenschicht zu einer Principal Component Analysis. Wir stellen fest: Hat das Encoding nur zwei Dimensionen, so ist die Rekonstruktion sehr schlecht (d.h. $\\vec{x}^{(i)}_p$ weicht stark von $\\vec{x}^{(i)}$ ab). Es würden hier relativ viele Dimensionen für das Encoding benötigt, um den Rekonstruktion wieder korrekt zu erstellen. Im folgenden wollen wir den Encoder/Decoder daher verbessern, indem wir ein tiefe neuronale Netze mit Convolutional Layern verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  \n",
    "\n",
    "\n",
    "# ENCODER-\n",
    "input_img = Input(shape=img_shape)\n",
    "\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "z = Dense(2)(x)\n",
    "\n",
    "encoder = Model(input_img,z)\n",
    "\n",
    "# DECODER\n",
    "decoder_input = Input(K.int_shape(z)[1:])\n",
    "x = Dense(np.prod(shape_before_flattening[1:]),activation='relu')(decoder_input)\n",
    "x = Reshape(shape_before_flattening[1:])(x)\n",
    "x = Conv2DTranspose(32, 3,padding='same', activation='relu', strides=(2, 2))(x)\n",
    "out = Conv2D(1, 3,padding='same', activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(decoder_input, out)\n",
    "z_decoded = decoder(z)\n",
    "\n",
    "dae = Model(input_img, z_decoded)\n",
    "dae.summary()\n",
    "decoder.summary()\n",
    "# Either train ..\n",
    "#dae.compile(optimizer='rmsprop', loss='mse')\n",
    "#dae.fit(x_train,x_train,epochs=10,batch_size=50)\n",
    "#dae.save_weights('dae_weights_2.h5')\n",
    "\n",
    "#... oder load weight\n",
    "dae.load_weights('dae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der (in diesem Falle zweidimensionale Vektor-) Raum, welcher die Encodings der Inputs enthält, wird auch als\n",
    "**Latent Space** bezeichnet. Wir visualisieren diesen, indem wir den Encoder-Teil der Netzwerkarchitektur\n",
    "auf die Test-Daten anwenden. Wir erhalten folgenden Latent Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = encoder.predict(x_test)\n",
    "plt.scatter(latent_space[:,0],latent_space[:,1],c=y_test,cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[L,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(dae.predict(x_train[L,:].reshape(1,28,28,1)).reshape(28,28),cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_train[L,:].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl der Latent space nun (auch im Vergleich zum obigen Beispiel eines einfaches Autoencoders) sehr niederdimesional ist und eine sehr gute Rekonstruction erlaubt, hat er dennoch auch Nachteile: der Latent Space\n",
    "ist unzusammendhängend und strukturlos. Diesen Umstand versucht ein sogenannter Variational Autoencoder zu berichtigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder\n",
    "\n",
    "Die **Variational Autoencoder** zählen zur Klasse der sogenannten **Generative Models**: Modelle, sie erlauben, ausgehend von einem zusammenhängen und strukturierten Latent Space entweder neue Bilder zu erzeugen bzw. bestehende Bilder gezielt zu verändern. \n",
    "\n",
    "<img src=\"variational_autoencoder.png\" height=\"100\" width=\"600\"/>\n",
    "\n",
    "Folgende Schritte werden bei einem VAE durchgeführt\n",
    "\n",
    "(1) Der Input wird codiert in einen Mittelwert $\\mu$ und Standartabweichung $\\sigma$\n",
    "\n",
    "$$\n",
    "\\vec{\\mu}, \\vec{\\sigma} = \\mathrm{encoder}(\\mathrm{input\\_img})\n",
    "$$\n",
    "\n",
    "(2) Von der Gaußverteilung $g(\\mu,\\sigma)$ wird dann zufällig Vektor $\\vec{z}$ des Latent Spaces ausgewählt:\n",
    "\n",
    "$$\n",
    "\\vec{z} = \\vec{\\mu} + \\vec{\\sigma} \\cdot \\vec{\\epsilon}\n",
    "$$\n",
    "\n",
    "(3) Dieser Vektor $\\vec{z}$ wird dann zurücktransformiert\n",
    "\n",
    "$$\n",
    "\\mathrm{reconstructed\\_img} = \\mathrm{decoder}(\\vec{z})\n",
    "$$\n",
    "\n",
    "(4) Wie wüblich wird ein Model erstellt\n",
    "\n",
    "$$\n",
    "\\mathrm{model} = \\mathrm{Model}(\\mathrm{input\\_img}, \\mathrm{reconstructed\\_img})\n",
    "$$\n",
    "\n",
    "(5) Dieses Model wird mit dann mit folgender Costfunction (bestehend aus reconstruction loss and regularization loss) trainiert\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}) = \n",
    "\\frac{1}{2m}\\sum_{i=1}^m\\left(\\vec{x}^{(i)}-\\vec{x}_{p}^{(i)}\\right)^2\n",
    "+\\sum_{i=1}^{\\mathrm{latent\\_dim}}\\mu_i^2+\\sigma_i^2-\\log(\\sigma_i^2)-1 \n",
    "$$\n",
    "\n",
    "Diese Costfunction besteht aus zwei Teilen:\n",
    "- Der erste Anteil versucht (wie schon bei den Autoencodern) den Input möglichst gut auf den Output zu matchen\n",
    "- Der zweite Anteil versucht, dass jedes Trainingsbespiel nicht von **einem** Punkt des Latent Spaces entsteht, sondern von einer Gaußverteilung mit $\\vec{\\mu}=0$ und $\\vec{\\sigma}=1$. Dieser Anteil versucht eine möglichst hohe Struktur in den Latent Space zu bringen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\mathbf{W}) = \n",
    "\\frac{1}{2m}\\sum_{i=1}^m\\left(\\vec{x}^{(i)}-\\vec{x}_{p}^{(i)}\\right)^2\n",
    "+\\sum_{i=1}^{\\mathrm{latent\\_dim}}\\mu_i^2+\\sigma_i^2-\\log(\\sigma_i^2)-1 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 12:55:37.855664 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0709 12:55:37.860863 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0709 12:55:37.867327 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0709 12:55:37.950902 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0709 12:55:38.035674 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0709 12:55:38.040637 140338461648704 deprecation.py:323] From /opt/tljh/user/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0709 12:55:38.096751 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0709 12:55:38.098166 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0709 12:55:38.099329 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0709 12:55:38.162178 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0709 12:55:38.163834 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0709 12:55:38.235125 140338461648704 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  \n",
    "\n",
    "#########\n",
    "# ENCODER\n",
    "#########\n",
    "input_img = Input(shape=img_shape)\n",
    "\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "#########\n",
    "# DECODER\n",
    "#########\n",
    "decoder_input = Input(K.int_shape(z)[1:])\n",
    "x = Dense(np.prod(shape_before_flattening[1:]),activation='relu')(decoder_input)\n",
    "x = Reshape(shape_before_flattening[1:])(x)\n",
    "x = Conv2DTranspose(32, 3,padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = Conv2D(1, 3,padding='same', activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(decoder_input, x)\n",
    "z_decoded = decoder(z)\n",
    "\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "# Either train ...\n",
    "#vae.compile(optimizer='rmsprop', loss=None)\n",
    "#vae.fit(x=x_train, y=None,shuffle=True,epochs=10,batch_size=batch_size,validation_data=(x_test, None))\n",
    "#vae.save_weights('va_weights.h5')\n",
    "\n",
    "# ... oder load weights\n",
    "vae.load_weights('vae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(decoder.predict(np.array([[-0.0,2.0]])).reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Model(input_img,z_mean)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pred = encoder.predict(x_test[:,:].reshape(-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mu_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40b0affc27ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mu_pred' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(mu_pred[:,0],mu_pred[:,1],c=y_test,cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
