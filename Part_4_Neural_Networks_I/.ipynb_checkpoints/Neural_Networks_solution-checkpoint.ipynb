{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung Neuronale Netze I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronale Netze an einem generischen Datenset\n",
    "\n",
    "Im Folgenden wollen wir ein Neuronales Netzwerk mit einem Hiddenlayer trainieren.\n",
    "Wählen Sie zuerst eines der beiden folgenden (nicht linear trennbaren) Training Sets aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2 # Anzahl Features\n",
    "N = 500 # Anzahl Datenpunkte pro Klasse \n",
    "m = 2*N # Anzahl Training Examples\n",
    "K=2 # Anzahl Klassen\n",
    "\n",
    "X = np.random.rand(2*N,2)*2-1;\n",
    "y = np.zeros(2*N, dtype='uint8')\n",
    "y.shape\n",
    "\n",
    "ind = np.reshape(np.where(X[:,0]**2<X[:,1]),-1)\n",
    "y[ind] = 1\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "\n",
    "X = X.T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2 # Anzahl Features\n",
    "K = 3 # Anzahl Klassen\n",
    "N = 100 # Anzahl Datenpunkte pro Klasse\n",
    "\n",
    "m = N*K # Anzahl Training Examples\n",
    "\n",
    "X = np.zeros((m,n)) # data matrix (each row = single example)\n",
    "y = np.zeros(m, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + 0.1*np.random.randn(N) # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "X = X.T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden sollen Sie ein Neuronales Netz einem dieser Training Sets trainieren. \n",
    "\n",
    "- Initialisieren Sie $\\mathbf{W}_1,\\mathbf{W}_2$ und $\\vec{b}_1,\\vec{b}_2$.\n",
    "- Erstellen Sie die Matrix $\\mathbf{Y}$.\n",
    "- In jedem Schritt des Gradient Descent Verfahrens führen Sie zuerst  einen Forward Pass zu Berechnung von $\\mathbf{H}$ aus. Im Anschluss wird der Backpropagation Algorithmus zur Bestimmung von $D\\mathbf{W}_1, D\\mathbf{W}_2, D\\vec{b}_1, D\\vec{b}_2$ verwendet.\n",
    "- Berechnen Sie nach Konvergenz des Gradient Descent Verfahrens wie üblich die Accuracy (hier nur bezüglich Training Set).\n",
    "- Plotten Sie die Cost-Function $J$ als Funktion des Iterationsschrittes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl Neuronen im Hidden Layer\n",
    "h = 10\n",
    "# Initialisierung W1, W2, b1, b2\n",
    "W1 = np.random.randn(h,n)*0.01\n",
    "W2 = np.random.randn(K,h)*0.01\n",
    "b1 = np.zeros((h,1))\n",
    "b2 = np.zeros((K,1))\n",
    "\n",
    "# Erstellung der Matrix Y\n",
    "Y = np.zeros((K,m))\n",
    "Y[y,range(m)] = 1\n",
    "\n",
    "# J_plot\n",
    "J = []\n",
    "\n",
    "# loop over epochs\n",
    "iterations = 20000\n",
    "alpha = 0.003\n",
    "for i in range(iterations):\n",
    "    ##############\n",
    "    # Forward Pass\n",
    "    ##############\n",
    "    # Calculate Z1, A1, Z2, A2=H und J\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.maximum(0,Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    H = np.exp(Z2)/np.sum(np.exp(Z2),axis=0,keepdims=True)\n",
    "    J.append(-sum(np.log(H[y,range(m)])))\n",
    "    \n",
    "    #################\n",
    "    # Backpropagation\n",
    "    #################\n",
    "    # Calculate Delta_2 and DW2, Db2\n",
    "    Delta_2 = H-Y\n",
    "    DW2 = np.dot(Delta_2,A1.T)\n",
    "    Db2 = np.sum(Delta_2,axis=1,keepdims=True)\n",
    "    \n",
    "    # Calculate Delta_1 and dW1\n",
    "    Delta_1 = np.dot(W2.T,Delta_2)\n",
    "    Delta_1[Z1<=0] = 0\n",
    "    DW1=np.dot(Delta_1,X.T)\n",
    "    Db1 = np.sum(Delta_1,axis=1,keepdims=True)\n",
    "\n",
    "    W2 += -alpha*DW2\n",
    "    W1 += -alpha*DW1\n",
    "    b2 += -alpha*Db2\n",
    "    b1 += -alpha*Db1\n",
    "        \n",
    "\n",
    "   \n",
    "# Berechne die Accuracy des Training Sets\n",
    "z1 = np.dot(W1,X) + b1\n",
    "a1 = np.maximum(0,Z1)\n",
    "z2 = np.dot(W2,A1)+b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotten Sie die Daten von oben zusammen mit den Decision Boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boundaries\n",
    "S=100\n",
    "X_p,Y_p = np.meshgrid(np.linspace(-1.0,1.0,S),np.linspace(-1.0,1.0,S))\n",
    "X_p = X_p.reshape(1,S**2)\n",
    "Y_p = Y_p.reshape(1,S**2)\n",
    "X_p = np.vstack([X_p,Y_p])\n",
    "z1 = np.dot(W1,X_p) + b1\n",
    "a1 = np.maximum(0,z1)\n",
    "z2 = np.dot(W2,a1)+b2\n",
    "predicted_class = np.argmax(z2, axis=0)\n",
    "print(predicted_class.shape)\n",
    "plt.scatter(X_p[0, :], X_p[1, :], c=predicted_class, s=40, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[0,:], X[1, :], c=y)\n",
    "plt.show()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nutzen nun die erste Training Set mit K=2 Klassen und verwenden nur zwei Neuronen im Hidden Layer ($h=2$). Trainieren das Neuronale Netz und fertigen Sie danach einen Scatterplot der beiden Komponenten $a_1$ und $a_2$ des Aktivierungsvektors\n",
    "$$\n",
    "\\vec{a}_1 = \\begin{pmatrix} a_1 \\\\ a_2\\end{pmatrix}\n",
    "$$\n",
    "im Hiddenlayer an. Sie sollten erkennen, dass die Daten linear separierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z1 = np.dot(W1,X) + b1\n",
    "#A1 = np.maximum(0,Z1)\n",
    "\n",
    "plt.scatter(A1[0,:], A1[1, :], c=y)\n",
    "plt.show()\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
