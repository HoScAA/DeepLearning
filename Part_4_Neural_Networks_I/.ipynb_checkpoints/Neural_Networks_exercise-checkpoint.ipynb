{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung Neuronale Netze I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronale Netze an einem generischen Datenset\n",
    "\n",
    "Im folgenden wollen wir ein Neuronales Netzwerk mit einem Hiddenlayer\n",
    "trainieren. Wählen Sie zuerst eines der beiden folgenden (nicht linear trennbaren) Training Sets aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2 # Anzahl Features\n",
    "N = 500 # Anzahl Datenpunkte pro Klasse \n",
    "m = 2*N # Anzahl Training Examples\n",
    "K=2 # Anzahl Klassen\n",
    "\n",
    "X = np.random.rand(2*N,2)*2-1;\n",
    "y = np.zeros(2*N, dtype='uint8')\n",
    "y.shape\n",
    "\n",
    "ind = np.reshape(np.where(X[:,0]**2<X[:,1]),-1)\n",
    "y[ind] = 1\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "\n",
    "X = X.T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2 # Anzahl Features\n",
    "K = 3 # Anzahl Klassen\n",
    "N = 100 # Anzahl Datenpunkte pro Klasse\n",
    "\n",
    "m = N*K # Anzahl Training Examples\n",
    "\n",
    "X = np.zeros((m,n)) # data matrix (each row = single example)\n",
    "y = np.zeros(m, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + 0.1*np.random.randn(N) # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "X = X.T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tainieren Sie das Neuronale Netz wir in Vorlesung beschrieben durch folgende Schritte: \n",
    "\n",
    "- Initialisieren Sie $\\mathbf{W}_1,\\mathbf{W}_2$ und $\\vec{b}_1,\\vec{b}_2$.\n",
    "- Erstellen Sie die Target-Matrix $\\mathbf{Y}$.\n",
    "- In jedem Schritt des Gradient Descent Verfahrens führen Sie zuerst  einen Forward Pass zu Berechnung von $\\mathbf{H}$ aus. Im Anschluss wird der Backpropagation Algorithmus zur Bestimmung von $D\\mathbf{W}_1, D\\mathbf{W}_2, D\\vec{b}_1, D\\vec{b}_2$ verwendet.\n",
    "- Berechnen Sie nach Konvergenz des Gradient Descent Verfahrens wie üblich die Accuracy (hier nur bezüglich Training Set).\n",
    "- Optional: Plotten Sie die Cost-Function $J$ als Funktion des Iterationsschrittes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Klassen (wird oben bei den Training Sets festgelegt)\n",
    "\n",
    "# Anzahl h der Neuronen im Hidden Layer \n",
    "################\n",
    "# Your Code here\n",
    "################\n",
    "h=20\n",
    "\n",
    "# Initialisierung W1, W2, b1, b2\n",
    "################\n",
    "# Your Code here\n",
    "################\n",
    "W1=np.random.randn(h,n)*0.01\n",
    "W2=np.random.randn(K,h)*0.01\n",
    "b1=np.zeros((h,1))\n",
    "b2=np.zeros((K,1))\n",
    "\n",
    "# Erstellung der Matrix Y\n",
    "################\n",
    "# Your Code here\n",
    "################\n",
    "Y=np.zeros((K,m))\n",
    "Y[y,range(m)] = 1\n",
    "\n",
    "# Zum Plotten der Costfunction J\n",
    "J = []\n",
    "\n",
    "# Iterations und learning rate\n",
    "iterations = 20000\n",
    "alpha = 0.003\n",
    "\n",
    "for i in range(iterations):\n",
    "    ################\n",
    "    # Forward Pass #\n",
    "    ################\n",
    "    # Berechne Z1, A1, Z2, A2=H und J\n",
    "    ################\n",
    "    # Your Code here\n",
    "    ################\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Backpropagation #\n",
    "    ###################\n",
    "    # Berechne Delta_2 und DW2, Db2\n",
    "    ################\n",
    "    # Your Code here\n",
    "    ################\n",
    "\n",
    "\n",
    "    # Berechne Delta_1 und DW1, Db1\n",
    "    ################\n",
    "    # Your Code here\n",
    "    ################\n",
    "\n",
    "    # Führe Parameter-Update durch\n",
    "    ################\n",
    "    # Your Code here\n",
    "    ################\n",
    "\n",
    "# Berechne die Accuracy des Training Sets\n",
    "predicted_classes=np.argmax(Z2,axis=0)\n",
    "print('accuracy:',np.mean(predicted_classes==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(J_plot)\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotten Sie die Daten von oben zusammen mit den Decision Boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boundaries\n",
    "S=100\n",
    "X_p,Y_p = np.meshgrid(np.linspace(-1.0,1.0,S),np.linspace(-1.0,1.0,S))\n",
    "X_p = X_p.reshape(1,S**2)\n",
    "Y_p = Y_p.reshape(1,S**2)\n",
    "X_p = np.vstack([X_p,Y_p])\n",
    "################\n",
    "# Your Code here\n",
    "################\n",
    "Z1=np.dot(W1,X_p)+b1\n",
    "a1=np.maximum(0,Z1)\n",
    "Z2=np.dot(W2,a1)+b2\n",
    "predicted_classes=np.argmax(Z2,axis=0)\n",
    "plt.scatter(X_p[0, :], X_p[1, :], c=predicted_classes, s=40, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[0,:], X[1, :], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nutzen nun die erste Training Set mit K=2 Klassen und verwenden nur zwei Neuronen im Hidden Layer ($h=2$). Trainieren das Neuronale Netz und fertigen Sie danach einen Scatterplot der beiden Komponenten $a_1$ und $a_2$ des Aktivierungsvektors\n",
    "$$\n",
    "\\vec{a}_1 = \\begin{pmatrix} a_1 \\\\ a_2\\end{pmatrix}\n",
    "$$\n",
    "im Hiddenlayer an. Sie sollten erkennen, dass die Daten linear separierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Your Code here\n",
    "################\n",
    "plt.scatter(A1[0,:], A1[1, :],c=y)\n",
    "plt.show()\n",
    "a1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
