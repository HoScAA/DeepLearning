{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification/ Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mehrklassige Klassifikation ist die Verallgemeinerung der binären Klassifikation auf $K$ Klassen. Sie ebenfalls zur Klasse der Supervised Machine Learning Probleme. Die Targets nemen hier $K$ Werte, z.B. $0,1,\\ldots,K-1$ an.\n",
    "Beispiele hierfür sind:\n",
    "\n",
    "- Unterscheidung der Handgeschriebenen Zahlen 0 bis 9\n",
    "<img src=\"MNIST.png\" height=\"100\" width=\"450\"/>\n",
    "\n",
    "\n",
    "\n",
    "Wir gehen im folgeden von $m$ Training Examples der Form\n",
    "$$\n",
    "(\\vec{x}^{(i)}, y^{(i)}), \\quad \\vec{x}^{(i)}\\in\\mathbb{R}^n,\\, y^{(i)}\\in\\{0,1\\ldots,K-1\\}\n",
    "$$\n",
    "aus.\n",
    "\n",
    "Bilder (wie im Beispiel oben) sind originäre Matrizen und werden zeilen- bzw. spaltenweise in einen Vektor geschrieben. Für die Klassifikation mit K Kassen verwenden wie eine Datenmatrix, welche die Training Examples \n",
    "als Zeilenvektoren enthält, d.h.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "\\left(\\vec{x}^{(1)} \\right)^T  \\\\ \n",
    "\\left(\\vec{x}^{(2)} \\right)^T \\\\\n",
    "\\vdots \\\\\n",
    "\\left(\\vec{x}^{(m)} \\right)^T\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{m\\times n}\n",
    "$$\n",
    "\n",
    "\n",
    "Für die Cost-Funktion führen wir zudem folgende sogenannte **One Hot Encoding** Nomenklatur für die Labels ein\n",
    "(hier explizit für $K=4$ gezeigt).\n",
    "$$\n",
    "\\mathrm{Klasse}\\,0 \\rightarrow \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0  \\end{pmatrix},\n",
    "\\quad\n",
    "\\mathrm{Klasse}\\,1 \\rightarrow \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0  \\end{pmatrix},\n",
    "\\qquad \n",
    "\\mathrm{Klasse}\\,2 \\rightarrow \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0  \\end{pmatrix}\\,\n",
    "\\qquad \n",
    "\\mathrm{Klasse}\\,3 \\rightarrow \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1  \\end{pmatrix}\\,.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothese und Costfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Hypothese besteht hier aus einem $K$-komponentigen Vektor $\\vec{h}_{\\vec{w}}(\\vec{x})$ und nimmt hier folgende Gestalt an\n",
    "$$\n",
    "\\vec{h}_{\\mathbf{W}}(\\vec{x}) = \n",
    "\\begin{pmatrix} \n",
    "\\frac{e^{\\vec{x}^T\\vec{w}_1}}{\\sum\\limits_{l=1}^K e^{\\vec{x}^T\\vec{w}_l}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{e^{\\vec{x}^T\\vec{w}_K}}{\\sum\\limits_{l=1}^K e^{\\vec{x}^T\\vec{w}_l}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "mit den Komponenten\n",
    "$$\n",
    "\\left(h_{\\mathbf{W}}(\\vec{x})\\right)_j = \n",
    "\\frac{e^{\\vec{x}^T\\vec{w}_j}}{\\sum\\limits_{l=1}^K e^{\\vec{x}^T\\vec{w}_l}} \n",
    "$$ \n",
    "und der Gewichsmatrix \n",
    "$$\n",
    "\\mathbf{W} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{w}_{1}, \\vec{w}_{2} \\ldots ,\\vec{w}_{K} \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n\\times K}\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Im Falle der Klassifikation mit $K$ Klassen stellen wir folgende **Costfunction** oder auch **Cross Entropy** auf\n",
    "$$\n",
    "J(\\mathbf{W})  = -\\sum_{i=1}^m\\sum_{j=1}^{K} Y_{ij}\\log H_{ij}\n",
    "$$\n",
    "\n",
    "mit den beiden Matrizen $H,Y \\in \\mathbb{R}^{m\\times K}$\n",
    "\n",
    "$$\n",
    "Y_{ij} = \\begin{cases} 1 & y^{(i)} = j \\\\ 0 & \\mathrm{sonst} \\end{cases},\\qquad H_{ij} =  \\frac{e^{\\left(\\vec{x}^{(i)} \\right)^T \\vec{w}_j}}{\\sum\\limits_{l=1}^K e^{\\left(\\vec{x}^{(i)} \\right)^T\\vec{w}_l}} \n",
    "$$\n",
    " \n",
    "Die Softmax-Regression kann nur lineare Decision Boundaries vorhersagen, wie folgende beide Beispiele (siehe\n",
    "Übung) illustrieren.\n",
    "<img src=\"multiclass_classification.png\" height=\"400\" width=\"800\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Der Gradient der Costfunction berechnet sich in Matrix-Notation zu\n",
    "$$\n",
    "\\vec{\\nabla}J(\\mathbf{W}) = \\mathbf{X}^T\\left(\\mathbf{H}-\\mathbf{Y}\\right)\n",
    "$$\n",
    "Wie üblich wird das Gradient Descent Verfahren\n",
    "$$\n",
    "\\mathbf{W} \\rightarrow \\mathbf{W} - \\alpha \\vec{\\nabla}J(\\mathbf{W})\n",
    "$$\n",
    "bis zur Konvergenz durchgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision und Recall\n",
    "\n",
    "Wir rufen uns die Confusion Matrix in Erinnerung. Jede Zeile dieser Matrix repräsentiert die Instanzen der tatsächlichen Klasse, während jede Spalte die Instanzen der vorhergesagten Klasse repräsentiert (oder umgekehrt). Betrachten Sie für die folgende Argumentation am besten diese Demo: https://ml4a.github.io/demos/confusion_mnist/.\n",
    "Wir betrachten eine Ziffer, z.B. die 3. Es gibt folgende Definitionen:\n",
    "\n",
    "- True Positive (TP) - \"Richtig, Positiv vorhergesagt\" $\\rightarrow$ tatsächlich: $=3$, vorhergesagt: $=3$ $\\Rightarrow$ Diagonalelement $\\left(3,3\\right)$: $C_{3,3}$\n",
    "- False Positive (FP) - \"Falsch, Positiv vorhergesagt\" $\\rightarrow$ tatsächlich: $\\neq3$, vorhergesagt: $=3$ $\\Rightarrow$ Spalte $3$ ohne Diagonalelement $\\left(3,3\\right)$: $C_{i,3}\\forall i\\neq3$\n",
    "- False Negative (FN) - \"Falsch, Negativ vorhergesagt\" $\\rightarrow$ tatsächlich: $=3$, vorhergesagt: $\\neq3$ $\\Rightarrow$ Zeile $3$ ohne Diagonalelement $\\left(3,3\\right)$: $C_{3,j}\\forall j\\neq3$\n",
    "- True Negative (TN) - \"Richtig, Negativ vorhergesagt\" $\\rightarrow$ tatsächlich: $\\neq3$, vorhergesagt: $\\neq3$ $\\Rightarrow$ alles außer Spalte $3$ und Zeile $3$: $C_{i,j}\\forall i,j\\neq3$\n",
    "\n",
    "Es wird definiert\n",
    "\n",
    "$$\n",
    "\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}} \\rightarrow \\mathrm{Precision} = \\frac{\\mathrm{Diagonalelement}}{\\mathrm{Spaltensumme}} \\Rightarrow \\mathrm{Precision}_j = \\frac{C_{j,j}}{\\sum\\limits_{l=0}^{K-1} {C_{l,j}}}\n",
    "$$\n",
    "und\n",
    "$$\n",
    "\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}} \\rightarrow \\mathrm{Recall} = \\frac{\\mathrm{Diagonalelement}}{\\mathrm{Zeilensumme}} \\Rightarrow \\mathrm{Recall}_i = \\frac{C_{i,i}}{\\sum\\limits_{l=0}^{K-1} {C_{i,l}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification mittels sklearn\n",
    "\n",
    "Die entsprechenden Funktionen und Module können direkt aus dem Kapitel über logistische Regression/binary classification übernommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "mult_class = linear_model.LogisticRegression()\n",
    "mult_class = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerische Hinweise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erzeugen der Target-Matrix $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y = np.zeros((4,3))\n",
    "y = np.array([0,1,2,2])\n",
    "Y[range(4),y] = 1\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweise zur Generierung der Hypothesen-Matrix $\\mathbf{H}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [1 0 1 0]\n",
      " [2 1 3 1]]\n",
      "[[0.1        0.2        0.3        0.4       ]\n",
      " [0.5        0.         0.5        0.        ]\n",
      " [0.28571429 0.14285714 0.42857143 0.14285714]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3,4],[1,0,1,0],[2,1,3,1]])\n",
    "print(A)\n",
    "\n",
    "# Summation entlang der Spalten (axis=1)\n",
    "np.sum(A,axis=1,keepdims=True)\n",
    "\n",
    "# Teilen jeder Spalte durch diesen Vektor\n",
    "print(A/np.sum(A,axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweise zur Ausgabe der Prediction: Bestimmung der Indizes des Maximums eines arrays entlang einer Achse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [1 0 1 1]\n",
      " [2 1 3 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3,4],[1,0,1,1],[2,1,3,2]])\n",
    "print(A)\n",
    "np.argmax(A,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
