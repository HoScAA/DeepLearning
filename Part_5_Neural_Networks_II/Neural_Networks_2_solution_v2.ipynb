{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks II - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb DataSet - Binary Classification\n",
    "\n",
    "In dieser Übung betrachten wir das IMDb (Internet Movie Database) Dataset. Dieses enthält\n",
    "50.000 Movie Reviews, die hälftig postiv (y=1) bzw. negativ (y=0) ausfallen (siehe auch https://keras.io/datasets/ $\\rightarrow$ IMDB Movie reviews sentiment classification).\n",
    "\n",
    "- Lesen Sie die Daten ein, wobei Sie nur 1000 meist verwendeten Worte für jeden Review verwenden (option num_words). Die Daten sind in diesem Fall Python-Listen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=1000)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lesen Sie die Dictionary word_index ein, welches  \n",
    "$$\n",
    "\\mathrm{word}\\quad \\rightarrow \\quad \\mathrm{integer\\,index}\n",
    "$$\n",
    "zuordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Erstellen Sie daraus ein Dictionary reverse_word_index, welches \n",
    "$$\n",
    "\\mathrm{integer\\,index}\\quad \\rightarrow \\quad \\mathrm{word}\n",
    "$$\n",
    "zuordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(val,key) for (key,val) in word_index.items() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lesen Sie damit zur Illustration einen beliebigen Review aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index.get(100,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a movie review\n",
    "' '.join([reverse_word_index.get(i,'') for i in train_data[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Führen Sie ein One-Hot-Encoding der Training- und Testdaten durch. Erstellen Sie dazu\n",
    "jeweils eine $m\\times n$-Matrix, welche in jeder Zeile (=für jeden Review) jeweils eine 1 an der Index-Stelle enthält, an der ein Wort in dem entsprechenden Review vorkommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot_encode(data,n=1000):\n",
    "    results = np.zeros((len(data),1000))\n",
    "    for i,rev in enumerate(data): \n",
    "        results[i,rev] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = one_hot_encode(train_data)\n",
    "X_test  = one_hot_encode(test_data)\n",
    "y_train = np.asarray(train_labels)\n",
    "y_test = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitten Sie vom Testset die ersten 10.000 Daten ab für die Validierung während des Trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_test[:10000,:]\n",
    "y_val = y_test[:10000]\n",
    "X_test = X_test[10000:,:]\n",
    "y_test = y_test[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Erstellen Sie ein neuronales Netz mit zwei Zwischenschichten der Größe $h_1=h_2=16$ und verwenden Sie beim Ausgang die Aktivierung sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "n = 1000\n",
    "h1 = 16\n",
    "h2 = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(h1, activation='relu', input_dim=n))\n",
    "model.add(Dense(h2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compilieren Sie das Model mit den Optionen \n",
    "    -  loss='binary_crossentropy'\n",
    "    -  metrics=['accuracy']\n",
    "    \n",
    "und trainieren Sie anschliessend Ihr Model mit den oben festgelegten Traings- und Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=4, batch_size=5,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotten Sie die Accuracy von Training und Test Set über die Epochen und berechnen Sie die accuracy bezüglich des Test-Sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(model.predict_classes(X_val)==y_val.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST-Fashion Data Set (Multiclass Classification)\n",
    "\n",
    "Das MNIST-Fashion DataSet enthält 70.000 Bilder (60.000 Training, 10.000 Test) mit den 10 Kategorien:\n",
    "\n",
    "| Index | Item  |\n",
    "|---|-------------|\n",
    "|0\t| T-shirt/top |\n",
    "|1\t| Trouser     |\n",
    "|2\t| Pullover    |\n",
    "|3\t|Dress        |\n",
    "|4  |Coat         |\n",
    "|5\t|Sandal       |\n",
    "|6\t|Shirt        |\n",
    "|7\t|Sneaker      |\n",
    "|8\t|Bag          |\n",
    "|9\t|Ankle boot   |\n",
    "\n",
    "- Lesen Sie Trainings- und Testdatein ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "#from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zeigen Sie ein beliebiges Bild an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[2000],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Führen Sie analog zur Vorlesung ein Preprocessing der Daten durch, in dem Sie\n",
    "    - Für Trainings- und Testdaten die letzten beiden Dimensionen (der Größe $28\\times 28$) das Datentensors auf\n",
    "    eine Dimension (der Größe $28^2$) kontrahieren\n",
    "    - Die Pixelwerte von $[0,255]$ auf das Intervall $[0,1]$ abbilden\n",
    "    - Von den Trainingsdaten die ersten 5.000 für die Validierung absplitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie dann folgende Schritte aus\n",
    "\n",
    "- Erstellen Sie ein geeignetes Neuronales Netz für diese Multiclass Classification\n",
    "- Compilieren und trainieren Sie ihr Netz\n",
    "- Plotten Sie die Accuracy des Training- und Validierungssets über die Epochen\n",
    "- Berechnen Sie die Accuracy bezüglich des Testsets\n",
    "- Lassen Sie die Confusion-Matrix für das Testset ausgeben (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "n = 28*28\n",
    "h1 = 100\n",
    "h2 = 100\n",
    "K = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(h1, activation='relu', input_dim=n))\n",
    "model.add(Dense(h2, activation='relu'))\n",
    "model.add(Dense(K, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=8, batch_size=50,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['sparse_categorical_accuracy'],'o')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(model.predict_classes(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Data Set (Regression)\n",
    "Das Boston Housing Data Set gibt die Preise von Häusern in Boston als Funktion der 13 Features, die in der\n",
    "folgenden Tabelle gezeigt sind an.\n",
    "\n",
    "|feature | label | content|\n",
    "|--|--|--|\n",
    "|0  |CRI     |     per capita crime rate by town     |\n",
    "|1  |ZN      |    proportion of residential land zoned for lots over 25,000 sq.ft.|\n",
    "|2  |INDUS   |    proportion of non-retail business acres per town|\n",
    "|3  |CHAS    |    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)|\n",
    "|4  |NOX     |    nitric oxides concentration (parts per 10 million)|\n",
    "|5  |RM      |    average number of rooms per dwelling|\n",
    "|6  |AGE     |    proportion of owner-occupied units built prior to 1940|\n",
    "|7  |DIS     |    weighted distances to five Boston employment centres|\n",
    "|8  |RAD     |    index of accessibility to radial highways|\n",
    "|9  |TAX     |    full-value property-tax rate per \\$10,000|\n",
    "|10 |PTRATIO |  pupil-teacher ratio by town|\n",
    "|11 |B       | $1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town|\n",
    "|12 |MEDV    | Median value of owner-occupied homes in \\$1000's|\n",
    "\n",
    "- Lesen Sie die Daten ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bereinigen Sie Ihre Daten vom Mittelwert und skalieren Sie auf eine Varianz von 1. Berechnen dazu den den Mittelwert $\\vec{\\mu}$ und die Standardabweichung $\\vec{\\sigma}$ des Training Sets (für alle features über alles Trainingsbeispiele). Ziehen dann $\\mu$ von Training und Test Set ab und teilen Sie anschliessend durch $\\vec{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "X_train -= mean\n",
    "X_train /=std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /=std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generieren Sie ein neuronales Netz mit zwei Hiddenlayern der Größe $h_1=h_2=64$. Der Ausgabelayer erhält hier\n",
    "keine Aktivierung, das es sich um eine Regression handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "n = 13\n",
    "h1 = 64\n",
    "h2 = 64\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Dense(1, input_dim=n))\n",
    "model.add(Dense(h1, activation='relu', input_dim=n))\n",
    "model.add(Dense(h2, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kompilieren Sie das Model mit loss='mse' und  metrics=['mae'] und führen Sie anschliessend das Training\n",
    "mit allen Trainingsdaten über 80 Epochen mit einem batch_size von 16 durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=200,batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluieren Sie den MAE für das Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.predict(X_test))\n",
    "print(model.evaluate(X_train,y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
