{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks II\n",
    "\n",
    "Neurale Netze lassen sich sehr einach mittels \n",
    "\n",
    "- TensorFlow (https://www.tensorflow.org)\n",
    "- Keras (https://keras.io)\n",
    "\n",
    "erstellen, trainieren und anwenden. Keras ist dabei ein High-Level API (application programming interface), welches auf Tensorflow aufbaut. Diese Vorlesung führt daher zuerst die wichtigsten Schritte zur Erstellung\n",
    "und zum Training von Neuronalen Netzen mittels Tensorflow ein. Im zweiten Schritt werden wir dann Keras näher beleuchten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "Die folgenden Abschnitte geben eine Step-by-Step Anleitung für die Erstellung und das Training Neuronaler Netze mittels Tensorflow. Wir beschränken uns hierbei auf das absolute Minimum, da wir später eine deutliche einfachere Vorgehensweise mit Keras kennenlernen werden. Folgende Informationen/Schritte sind dabei notwendig\n",
    "\n",
    "(1) **Trainingsdaten**: $(\\vec{x}^{(i)},y^{(i)})$ mit $i=1,\\ldots,m$ und $\\vec{x}^{(i)}\\in\\mathbb{R}^n$\n",
    "\n",
    "(2) **Netzwerkarchitektur**: Die Schichten des Netzes haben die Größen $(n,h_1,h_2,\\ldots,h_L,K)$, wobei die Größe der Eingabeschicht der Anzahl der features $n$ und die Größe der Ausgabeschicht der Anzahl der Klassen $K$ entspricht. Die $L$ Zwischenlayer haben die Größe $h_1,\\ldots,h_K$. Zudem muss die Aktivierungsfunktion in jedem Layer spezifiziert werden.\n",
    "\n",
    "(3) **Costfunction**: Mehrklassige Klassifikation mit der Crossentropy \n",
    "$$ \n",
    "J(\\mathbf{W}_1,\\ldots,\\mathbf{W}_{K+1}) = -\\sum_{i=1}^n\\sum_{j=1}^K \\mathbf{Y}_{ij}\\log \\mathbf{H}_{ij}\n",
    "$$\n",
    "oder Regression mit\n",
    "$$ \n",
    "J(\\mathbf{W}_1,\\ldots,\\mathbf{W}_{K+1}) = \\frac{1}{m}\\sum_{i=1}^n(y^{(i)}_{\\mathrm{pred}}-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "(4) **Optimierverfahren**: Wir werden die learning rate $\\alpha$ und das Optimierverfahren (wir nutzen bei Keras das RMSprop Verfahren - siehe Vorlesung) festgelegt.\n",
    "\n",
    "(5) **Training**/**Evaluaring**:\n",
    "Für das Training der Netzes verwenden wir jeweils das Mini-Batch gradient descent Verfahren - siehe Vorlesung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Trainingsdaten\n",
    "\n",
    "Wir lesen als Trainingsdaten das MNIST Datenset ein und splitten die 70.000 Bilder in\n",
    "\n",
    "- 55.000: Training\n",
    "- 10.000: Test\n",
    "- 5.000: Validation\n",
    "\n",
    "Das bisher noch nicht angesprochene Validation Set wird beim Machine Learning zur Überprüfung der Hypterparameter (in unserem Falle sind dies\n",
    "die Anzahl der Hiddenlayer $L$ und deren Größen $h_1,\\ldots,h_L$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 70.000 Bilder \n",
    "# - 55.000 Training\n",
    "# - 10.000 Testing\n",
    "# - 5000 Validation\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Netzwerkarchitektur\n",
    "\n",
    "Im folgenden legen wir die Netzwerkarchitektur fest. Wir nutzen $L=1$\n",
    "mit $h_1=100$ (zudem ist bei MNIST $n=28^2=784$ und $K=10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 28*28  # MNIST\n",
    "h_1 = 100\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n))\n",
    "y = tf.placeholder(tf.int32, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X,h_1, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden1, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Costfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = tf.losses.sparse_softmax_cross_entropy(y,logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Optimierverfahren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Training/Evaluatierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Berechnug der accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Initalisiering der Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Festlegung der Iterationschritte und des batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)  Tensorflow session eröffnen und Training durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with tf.Session(config=config) as sess:\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for n in range(iterations):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})           \n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Iteration:\", n, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (e) Accuracy bezüglich Test Set berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") \n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "print(np.mean(y_pred==y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Mithilfe der High-Level API Keras lässt sich das obere Prozedere sehr einfach, elegant und nutzerfreundlich implementieren. Dazu importieren wir Keras und die wichtigsten Funktionen zur Erstellung des obigen Neuronalen Netzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir führen nun die oben genannten 5 Schritte durch:\n",
    "#### (1) Trainingsdaten: \n",
    "\n",
    "Die Trainingsdaten bleiben unverändert.\n",
    "\n",
    "X.shape = (m,n)\n",
    "\n",
    "y.shape = (m,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Netzwerkarchitektur:\n",
    "\n",
    "Das Model lässt sich hier sehr einfach mittels des Sequential Frameworks erstellen (https://keras.io/getting-started/sequential-model-guide/).\n",
    "\n",
    "Die zur Verfügung stehenden Aktivierungsfunktionen finden sich hier: https://keras.io/api/layers/activations/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 11:51:43.640750 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0604 11:51:43.649227 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0604 11:51:43.658878 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 109,810\n",
      "Trainable params: 109,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n = 28*28\n",
    "h1 = 100\n",
    "h2 = 100\n",
    "h3 = 100\n",
    "h4 = 100\n",
    "K = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(h1, activation='relu', input_dim=n))\n",
    "model.add(Dense(h2, activation='relu'))\n",
    "model.add(Dense(h3, activation='relu'))\n",
    "model.add(Dense(h4, activation='relu'))\n",
    "model.add(Dense(K, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) und (4) Costfunction und Optimierverfahren\n",
    "\n",
    "Wir wählen hier beispielhaft \n",
    "\n",
    "- Optimierer:  RMSprop aus https://keras.io/optimizers .  \n",
    "- Costfunction: Wir verwenden sparse_categorical_crossentropy aus https://keras.io/api/losses , welche Integer-Werte als Targets verarbeiten kann (die categorical_crossentropy arbeitet mit Targets in der One-Hot-Encoding Darstellung)\n",
    "- Metrik: accuracy aus https://keras.io/metrics ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.adam(),\n",
    "              loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Training/Evaluierung\n",
    "\n",
    "Für das Trainig verwenden wir fit Routine (https://keras.io/models/sequential/#fit), welcher wir die \n",
    "Trainingsdaten, die Anzahl der Epochen, den Mini-Batch-Size, sowie das Validierungs-Set übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 12:18:21.389195 139947246712640 deprecation.py:323] From /opt/tljh/user/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0604 12:18:21.421484 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0604 12:18:21.529806 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0604 12:18:21.639579 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0604 12:18:21.651882 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0604 12:18:21.653101 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0604 12:18:21.702075 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0604 12:18:21.704107 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0604 12:18:21.821369 139947246712640 module_wrapper.py:139] From /opt/tljh/user/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 8s 144us/step - loss: 0.2679 - sparse_categorical_accuracy: 0.9183 - val_loss: 0.1495 - val_sparse_categorical_accuracy: 0.9554\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 7s 135us/step - loss: 0.1215 - sparse_categorical_accuracy: 0.9625 - val_loss: 0.1026 - val_sparse_categorical_accuracy: 0.9698\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 7s 136us/step - loss: 0.0905 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.1163 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 7s 135us/step - loss: 0.0714 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.1023 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 7s 134us/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0916 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.0497 - sparse_categorical_accuracy: 0.9836 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9752\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.0448 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.0914 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 7s 133us/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0896 - val_sparse_categorical_accuracy: 0.9780\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 7s 135us/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0919 - val_sparse_categorical_accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 8s 138us/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.1063 - val_sparse_categorical_accuracy: 0.9750\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.1053 - val_sparse_categorical_accuracy: 0.9764\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9921 - val_loss: 0.1026 - val_sparse_categorical_accuracy: 0.9754\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.1100 - val_sparse_categorical_accuracy: 0.9774\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.1015 - val_sparse_categorical_accuracy: 0.9756\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 8s 140us/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 8s 141us/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1059 - val_sparse_categorical_accuracy: 0.9774\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 8s 141us/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0964 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 8s 141us/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.1193 - val_sparse_categorical_accuracy: 0.9762\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 8s 143us/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.1062 - val_sparse_categorical_accuracy: 0.9796\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 8s 141us/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.1017 - val_sparse_categorical_accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zur Evaluierung bzg. des Testsets verwenden wir die Funktion evaluate (https://keras.io/models/model/#evaluate) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ... oder predict_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_p = model.predict_classes(X_test)\n",
    "np.mean(y_test_p==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die Accuracy von Training- und Validierungset lässt sich wie folgt plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'],'o')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],'o')\n",
    "plt.plot(history.history['val_loss'],'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
