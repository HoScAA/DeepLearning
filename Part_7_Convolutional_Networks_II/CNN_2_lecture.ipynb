{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks II\n",
    "\n",
    "\n",
    "### Deep Convolutinal Neural Networks\n",
    "\n",
    "Mit den in der letzten Vorlesung eingeführten Operationen (Convolution, Pooling und Fully Connected Layer) können nun sehr tiefe (\"Deep Learning\") Faltungsnetzwerke für **Visual Recognition** erstellt werden. Als Beispiel  ist in der folgenden Abbildung VGG16-Netz (Karen Simonyan und Andrew Zisserman, Visual Geometry Group,  https://arxiv.org/pdf/1409.1556.pdf) gezeigt.\n",
    "\n",
    "<img src=\"VGG16.png\" height=\"100\" width=\"600\"/>\n",
    "\n",
    "Das Netz hat folgende Struktur:\n",
    "\n",
    "Der Eingang hat eine feste Größe von (224,224,3) als RGB-Bild. Das Bild wird durch einen Stack von Covolution Layer \n",
    "geleitet, wobei Filterkerne mit einem receptive field von (3,3) verwendet werden. Die Pooling Operation wird von fünf Max-Pooling-Layern durchgeführt (über ein Feld der Größe (2,2) und einen Stride S=2.\n",
    "\n",
    "Den Abschluss bilden drei Fully-Connected Layer  und eine Softmax Classification mit $K=1000$-Kassen.\n",
    "\n",
    "Keras hat einige vortrainierte Netze bereits an Board (siehe https://keras.io/applications/). Diese Modelle lassen sich sehr bequem einlesen und nutzen (hier am Beispiel das oben gezeigten VGG16 Netzes https://keras.io/applications/#vgg16 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[4].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup eines ConvNets mittels Keras\n",
    "\n",
    "Die Hauptbestandteile eines Convnets (Convolution und Pooling) lassen sich mittels der beiden in keras.layers enthaltenen Funktionen \n",
    "\n",
    "- Conv2D (https://keras.io/layers/convolutional/#conv2d)\n",
    "- MaxPooling2D (https://keras.io/layers/pooling/#maxpooling2d)\n",
    "\n",
    "Darüber hinaus wird am der der Convolution Blöcke einer sogenannter Flatten-Layer gebildet (dieser fasst schlicht den letzten Tensor der Convolution als Vektor zusammen), von dem ausgehend sich dann ein Fully-Connected Netzwerk befindet. \n",
    "\n",
    "- Flatten (https://keras.io/layers/core/#flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung von Convolutional Networks\n",
    "\n",
    "In der folgenden Veröffentlichung ist veranschaulicht, welche Strukturen die verschiedenen Convolutional Layer in einem CNN lernen. Abbildung 2 zeigt die Top 9 Aktivierungen aus\n",
    "einer zufällig ausgewählten Menge von Feature Maps bezüglich der Validierungsdaten, zurückgerechnet auf den Input.\n",
    "\n",
    "https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Backpropagation\n",
    "\n",
    "Das Training eines CNN erfolgt via Backprogapation analog zu den ANN, siehe z.B.:\n",
    "\n",
    "https://github.com/ZZUTK/An-Example-of-CNN-on-MNIST-dataset/blob/master/doc/Derivation_of_Backpropagation_in_CNN.pdf\n",
    "\n",
    "\n",
    "### Data Augmentation, Dropout und Local Response Normalization\n",
    "Um Overfitting zu vermeiden, werden bei ConvNets oft einige oder alle der folgeden Methoden verwendet\n",
    "\n",
    "- Data Augmentation\n",
    "- Dropout (https://arxiv.org/pdf/1207.0580.pdf)\n",
    "- Local Response Normalization (Section 3.3 in http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Sets\n",
    "\n",
    "Typische Datensätze zum Trainieren von CNN sind \n",
    "\n",
    "| Dataset      |  Number of Classes | URL                                        |\n",
    "|--------------|--------------------|------------------------------------------- |\n",
    "|  CIFAR-10    | 10                 |https://www.cs.toronto.edu/~kriz/cifar.html |\n",
    "|  CIFAR-100   | 100                |https://www.cs.toronto.edu/~kriz/cifar.html |\n",
    "|  ImageNet    | 20.000             |http://image-net.org/                       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Demo for MNIST\n",
    "\n",
    "Unter dem folgenden Link findet sich die Animation eines auf MNIST-Daten\n",
    "trainierten ConvNets:\n",
    "\n",
    "https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
